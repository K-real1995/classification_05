"""
───────────────────────────────────
Классификация ирисов с помощью алгоритма KNN (K ближайших соседей).

Цель работы
-----------
Ранее мы обучили модель KNN на 2 признаках (длина лепестка + ширина лепестка).
Здесь мы добавляем третий признак и смотрим, как это влияет на точность:

  * Датасет 1 — длина чашелистика + длина лепестка + ширина лепестка
  * Датасет 2 — ширина чашелистика + длина лепестка + ширина лепестка
  * Базовый   — длина лепестка    + ширина лепестка  (как в уроке)

Дополнительно перебираем n_neighbors от 1 до 20, чтобы найти лучшее значение k,
и строим 3D-графики для визуализации данных.
"""

from __future__ import annotations

# numpy — библиотека для работы с массивами и математическими операциями
import numpy as np

# matplotlib — библиотека для построения графиков и визуализации данных
import matplotlib.pyplot as plt

# load_iris — функция, которая загружает знаменитый датасет Ирисов Фишера
# (150 цветков ирисов трёх видов с 4 измеренными параметрами у каждого)
from sklearn.datasets import load_iris

# train_test_split — разбивает данные на тренировочную и тестовую выборки
# (модель учится на тренировочной, а качество проверяем на тестовой)
from sklearn.model_selection import train_test_split

# KNeighborsClassifier — алгоритм KNN (K ближайших соседей).
# Идея простая: чтобы определить класс нового объекта, смотрим на k ближайших
# к нему соседей в тренировочных данных и выбираем самый частый класс среди них.
from sklearn.neighbors import KNeighborsClassifier

# accuracy_score — считает долю правильных ответов (точность модели)
# classification_report — выводит подробный отчёт: precision, recall, f1-score
from sklearn.metrics import accuracy_score, classification_report

# ──────────────────────────────────────────────
# Константы (настройки, которые легко менять в одном месте)
# ──────────────────────────────────────────────

# random_state — «зерно» генератора случайных чисел.
# Если поставить одно и то же число, разбивка данных будет воспроизводимой:
# каждый запуск даст одинаковый результат. Это важно для отладки.
RANDOM_STATE = 17

# Количество соседей по умолчанию (стандартное значение для KNN)
K_DEFAULT = 5

# Диапазон значений k, которые будем перебирать: от 1 до 20 включительно
K_RANGE = range(1, 21)


# ──────────────────────────────────────────────
# Вспомогательные функции
# ──────────────────────────────────────────────

def train_and_evaluate(
    X_train: np.ndarray,
    X_test: np.ndarray,
    y_train: np.ndarray,
    y_test: np.ndarray,
    n_neighbors: int = K_DEFAULT,
) -> tuple[KNeighborsClassifier, float]:
    """
    Обучает модель KNN и возвращает (обученную модель, точность).

    Параметры
    ---------
    X_train : массив признаков для обучения
    X_test  : массив признаков для проверки
    y_train : правильные ответы (метки классов) для обучения
    y_test  : правильные ответы для проверки
    n_neighbors : количество соседей (k)

    Возвращает
    ----------
    model    : обученная модель KNN
    accuracy : доля правильных ответов на тестовой выборке (от 0.0 до 1.0)
    """
    # Создаём модель KNN с заданным числом соседей
    model = KNeighborsClassifier(n_neighbors=n_neighbors)

    # Обучаем модель: она запоминает тренировочные данные
    # (KNN буквально «запоминает» все точки и потом ищет ближайшие)
    model.fit(X_train, y_train)

    # Делаем предсказания на тестовых данных и считаем точность
    accuracy = accuracy_score(y_test, model.predict(X_test))

    return model, accuracy


def find_best_k(
    X_train: np.ndarray,
    X_test: np.ndarray,
    y_train: np.ndarray,
    y_test: np.ndarray,
    k_range: range = K_RANGE,
) -> tuple[list[int], float, list[float]]:
    """
    Перебирает все значения k из диапазона k_range и находит лучшее.

    Зачем это нужно?
    Если k слишком маленькое (например, 1), модель будет «переобучаться» —
    слишком сильно подстраиваться под шум в данных.
    Если k слишком большое, модель будет слишком «грубой» и не уловит закономерности.
    Нужно найти золотую середину.

    Возвращает
    ----------
    best_ks    : список лучших значений k (может быть несколько с одинаковой точностью)
    best_acc   : лучшая достигнутая точность
    accuracies : список точностей для каждого k (пригодится для графика)
    """
    # Начинаем с заведомо плохой точности, чтобы любой результат был лучше
    best_acc = -1.0
    best_ks: list[int] = []
    accuracies: list[float] = []

    # Перебираем каждое значение k от 1 до 20
    for k in k_range:
        # Обучаем модель с текущим k и получаем точность
        _, acc = train_and_evaluate(X_train, X_test, y_train, y_test, n_neighbors=k)
        accuracies.append(acc)

        if acc > best_acc:
            # Нашли новый лучший результат — обновляем
            best_acc = acc
            best_ks = [k]
        elif acc == best_acc:
            # Точность такая же, как у лучшего — добавляем k в список лучших
            best_ks.append(k)

    return best_ks, best_acc, accuracies


def plot_3d(
    data: np.ndarray,
    labels: np.ndarray,
    axis_labels: tuple[str, str, str],
    title: str,
) -> None:
    """
    Строит 3D-график (облако точек), где каждая точка — один цветок ириса.
    Цвет точки определяется видом ириса (setosa / versicolor / virginica).
    Три оси — три признака цветка.
    """
    # Создаём фигуру (холст) размером 9x7 дюймов
    fig = plt.figure(figsize=(9, 7))

    # Добавляем трёхмерные оси на холст
    ax = fig.add_subplot(111, projection="3d")

    # Рисуем точки: каждая точка = один цветок
    # data[:, 0] — первый признак (ось X)
    # data[:, 1] — второй признак (ось Y)
    # data[:, 2] — третий признак (ось Z)
    # c=labels — цвет определяется классом (видом ириса)
    scatter = ax.scatter3D(
        data[:, 0],
        data[:, 1],
        data[:, 2],
        c=labels,
        cmap="viridis",       # цветовая палитра
        alpha=0.8,            # прозрачность (0 = невидимый, 1 = непрозрачный)
        edgecolors="k",       # чёрная обводка у точек для контраста
        linewidths=0.3,       # толщина обводки
        s=50,                 # размер точек
    )

    # Подписываем оси
    ax.set_xlabel(axis_labels[0])
    ax.set_ylabel(axis_labels[1])
    ax.set_zlabel(axis_labels[2])

    # Заголовок графика
    ax.set_title(title, fontsize=13, fontweight="bold")

    # Цветовая шкала — показывает, какой цвет соответствует какому классу
    fig.colorbar(scatter, ax=ax, shrink=0.6, label="Класс")


def plot_k_accuracy(k_range: range, accuracies: list[float], dataset_name: str) -> None:
    """
    Строит график: как меняется точность модели при разных значениях k.
    По оси X — количество соседей (k), по оси Y — точность (accuracy).
    Помогает визуально найти лучшее значение k.
    """
    fig, ax = plt.subplots(figsize=(9, 5))

    # Рисуем линию с точками-маркерами
    ax.plot(list(k_range), accuracies, marker="o", linewidth=2, markersize=6)

    ax.set_xlabel("Количество соседей (k)", fontsize=12)
    ax.set_ylabel("Точность (Accuracy)", fontsize=12)
    ax.set_title(
        f"Зависимость точности KNN от k  —  {dataset_name}",
        fontsize=13,
        fontweight="bold",
    )

    # Отображаем каждое значение k на оси X
    ax.set_xticks(list(k_range))

    # Добавляем сетку для удобства чтения графика
    ax.grid(True, alpha=0.3)

    # Ограничиваем ось Y, чтобы различия были лучше видны
    ax.set_ylim(0.85, 1.02)


# ──────────────────────────────────────────────
# Главный конвейер (основная логика программы)
# ──────────────────────────────────────────────

def main() -> None:
    # =============================================
    # Шаг 1. Загружаем данные
    # =============================================
    # Датасет Ирисов содержит 150 цветков трёх видов:
    #   - setosa (0), versicolor (1), virginica (2)
    # У каждого цветка 4 признака:
    #   [0] sepal length — длина чашелистика (см)
    #   [1] sepal width  — ширина чашелистика (см)
    #   [2] petal length — длина лепестка (см)
    #   [3] petal width  — ширина лепестка (см)
    iris = load_iris()

    # X_full — таблица признаков (150 строк x 4 столбца)
    # y      — массив меток классов (150 значений: 0, 1 или 2)
    X_full, y = iris.data, iris.target

    # Названия признаков (пригодятся для подписей на графиках)
    feature_names = iris.feature_names

    # =============================================
    # Шаг 2. Готовим наборы признаков
    # =============================================
    # Ранее модель обучалась только на 2 признаках: длина и ширина лепестка.
    # Задача: добавить третий признак и посмотреть, станет ли модель точнее.

    # Базовый набор (как в уроке): только длина лепестка + ширина лепестка
    # Берём столбцы с индексами 2 и 3 (petal length, petal width)
    X_base = X_full[:, 2:4]

    # Датасет 1: длина чашелистика + длина лепестка + ширина лепестка
    # Для этого удаляем столбец 1 (ширина чашелистика) — он нам не нужен
    X_ds1 = np.delete(X_full, 1, axis=1)

    # Датасет 2: ширина чашелистика + длина лепестка + ширина лепестка
    # Для этого удаляем столбец 0 (длина чашелистика)
    X_ds2 = np.delete(X_full, 0, axis=1)

    # Собираем все три набора в словарь для удобного перебора в цикле
    datasets = {
        "Базовый (2 признака)": X_base,
        "Датасет 1 (дл. чашел. + дл. леп. + шир. леп.)": X_ds1,
        "Датасет 2 (шир. чашел. + дл. леп. + шир. леп.)": X_ds2,
    }

    # =============================================
    # Шаг 3. Обучаем и оцениваем модель на каждом наборе признаков
    # =============================================
    print("=" * 65)
    print("  KNN-классификация ирисов — сравнение наборов признаков")
    print("=" * 65)

    # Словарь для хранения результатов
    results: dict[str, float] = {}

    for name, X in datasets.items():
        # Разбиваем данные: 75% — обучение, 25% — тест
        # random_state фиксирует разбивку, чтобы результат был воспроизводимым
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, random_state=RANDOM_STATE,
        )

        # Обучаем KNN (k=5) и получаем точность
        model, acc = train_and_evaluate(X_train, X_test, y_train, y_test)
        results[name] = acc

        # Выводим результат
        print(f"\n>> {name}")
        print(f"   Точность (k={K_DEFAULT}): {acc:.4f}")

        # classification_report показывает подробную статистику по каждому классу:
        # - precision (точность) — какая доля предсказанных «класс X» реально «класс X»
        # - recall (полнота) — какую долю реальных «класс X» модель нашла
        # - f1-score — среднее гармоническое precision и recall
        print(
            f"   Подробный отчёт:\n"
            f"{classification_report(y_test, model.predict(X_test), target_names=iris.target_names)}"
        )

    # =============================================
    # Шаг 4. Ищем лучшее k для Датасета 1
    # =============================================
    # Перебираем k от 1 до 20 и смотрим, при каком k точность максимальна
    X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(
        X_ds1, y, random_state=RANDOM_STATE,
    )
    best_ks, best_acc, accs_ds1 = find_best_k(
        X_train_1, X_test_1, y_train_1, y_test_1, K_RANGE,
    )

    print("-" * 65)
    print(f"  Лучшие k для Датасета 1: {best_ks}  (точность = {best_acc:.4f})")
    print("-" * 65)

    # =============================================
    # Шаг 5. Ищем лучшее k для Датасета 2 (для полноты картины)
    # =============================================
    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(
        X_ds2, y, random_state=RANDOM_STATE,
    )
    best_ks_2, best_acc_2, accs_ds2 = find_best_k(
        X_train_2, X_test_2, y_train_2, y_test_2, K_RANGE,
    )

    print(f"  Лучшие k для Датасета 2: {best_ks_2}  (точность = {best_acc_2:.4f})")
    print("=" * 65)

    # =============================================
    # Шаг 6. Визуализация
    # =============================================

    # --- 3D-график для Датасета 1 ---
    # Каждая точка — цветок. Три оси — три признака. Цвет — вид ириса.
    # Если точки одного цвета группируются отдельно — модель легко их разделит.
    plot_3d(
        X_ds1, y,
        axis_labels=(
            feature_names[0],  # длина чашелистика
            feature_names[2],  # длина лепестка
            feature_names[3],  # ширина лепестка
        ),
        title="Датасет 1: дл. чашелистика + дл. лепестка + шир. лепестка",
    )

    # --- 3D-график для Датасета 2 ---
    plot_3d(
        X_ds2, y,
        axis_labels=(
            feature_names[1],  # ширина чашелистика
            feature_names[2],  # длина лепестка
            feature_names[3],  # ширина лепестка
        ),
        title="Датасет 2: шир. чашелистика + дл. лепестка + шир. лепестка",
    )

    # --- Графики зависимости точности от k ---
    # Помогают визуально найти оптимальное количество соседей
    plot_k_accuracy(K_RANGE, accs_ds1, "Датасет 1")
    plot_k_accuracy(K_RANGE, accs_ds2, "Датасет 2")

    # Автоматически подгоняем отступы, чтобы графики не наезжали друг на друга
    plt.tight_layout()

    # Показываем все графики на экране
    plt.show()


# Эта конструкция означает: «Запускай main() только если файл запущен напрямую,
# а не импортирован как модуль в другой файл».
# Это стандартная практика в Python.
if __name__ == "__main__":
    main()
